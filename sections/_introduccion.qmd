# Introducción
Para este proyecto, el grupo utilizará tres técnicas utilizadas en la minería de datos. Estas técnicas serán comparadas entre si con el fin de evaluar los algoritmos y poder encontrar aquel que resulte más eficiente en terminos del tiempo.

Antes de mostrar los algoritmos y sus resultados, se da una breve explicación de lo que es la minería de datos.

## ¿Qué es la minería de datos? [@MiningMassiveDatasets]
En la década de 1990, la “minería de datos” era un concepto nuevo, apasionante y popular. Alrededor de 2010, la gente empezó a hablar de “grandes datos”. Hoy en día, el término popular es "ciencia de datos". Sin embargo, durante todo este tiempo, el concepto siguió siendo el mismo: utilizar el hardware más potente, los sistemas de programación más potentes y los algoritmos más eficientes para resolver problemas en ciencia, comercio, atención sanitaria, gobierno, las humanidades y muchos otros campos de la ciencia y el esfuerzo humano.

## Modelado
Para muchos, la minería de datos es el proceso de creación de un modelo a partir de datos, a menudo mediante el proceso de aprendizaje automático. Sin embargo, de manera más general, el objetivo de la minería de datos es un algoritmo. En muchas aplicaciones importantes, la parte difícil es crear el modelo y, una vez que el modelo está disponible, el algoritmo para utilizarlo es sencillo.

**Ejemplo:** Considere el problema de detectar correos electrónicos que son ataques de phishing. El enfoque más común es crear un modelo de correos electrónicos de phishing, tal vez examinando correos electrónicos que las personas han reportado recientemente como ataques de phishing y buscando palabras o frases que aparecen con una frecuencia inusual en esos correos electrónicos, como “príncipe nigeriano” o “verificar cuenta”. El modelo podría ser ponderaciones de palabras, con ponderaciones positivas para las palabras que aparecen con frecuencia en correos electrónicos de phishing y ponderaciones negativas para las palabras que no aparecen. Entonces el algoritmo para detectar correos electrónicos de phishing es sencillo. Aplique el modelo a cada correo electrónico, es decir, sume los pesos de las palabras en ese correo electrónico y diga que el correo electrónico es phishing si y solo si la suma es positiva.

## Modelado estadístico
Los estadísticos fueron los primeros en utilizar el término "minería de datos". Originalmente, "minería de datos" o "dragado de datos" era un término despectivo que se refería a los intentos de extraer información que no estaba respaldada por los datos. Hoy en día, la "minería de datos2 ha adquirido un significado positivo. Ahora, los estadísticos ven la minería de datos como la construcción de un modelo estadístico, es decir, una distribución subyacente de la cual se extraen los datos visibles.

**Ejemplo:** Supongamos que nuestros datos son un conjunto de números. Estos datos son mucho más simple que los datos que se extraerían de ellos, pero servirá como ejemplo. Un estadístico podría decidir que los datos provienen de una distribución gaussiana y utilizar una fórmula para calcular los parámetros más probables de esta distribución gaussiana. La media y la desviación estándar de esta distribución gaussiana caracterizan completamente la distribución y se convertirían en el modelo de los datos.

## Aprendizaje automático
Hay quienes consideran que la minería de datos es sinónimo de aprendizaje automático. No hay duda de que parte de la minería de datos utiliza apropiadamente algoritmos de aprendizaje automático. Los profesionales del aprendizaje automático utilizan los datos como un conjunto de entrenamiento para entrenar un algoritmo de uno de los muchos tipos utilizados para el aprendizaje automático, como redes de Bayes, máquinas de vectores de soporte, árboles de decisión, modelos ocultos de Markov y una gran variedad de otros. 

Hay situaciones en las que tiene sentido utilizar datos de esta manera. El caso típico en el que el aprendizaje automático es un buen enfoque es cuando tenemos poca idea de lo que dicen los datos sobre el problema que intentamos resolver. Por ejemplo, no está claro qué es lo que hace que a ciertos espectadores les guste o no les gusten las películas. Así, al responder al "desafío de Netflix" de idear un algoritmo que prediga las calificaciones de las películas por parte de los usuarios, basándose en una muestra de sus respuestas, los algoritmos de aprendizaje automático han demostrado ser bastante exitosos.

Sin embargo, el aprendizaje automático puede resultar poco competitivo en situaciones en las que podemos describir los objetivos de la minería de forma más directa. Un **ejemplo** interesante es el intento de WhizBang! Lab. Laboratorios para utilizar el aprendizaje automático para localizar los currículums de las personas en la Web. No pudo hacerlo mejor que los algoritmos diseñados a mano para buscar algunas de las palabras y frases obvias que aparecen en el currículum típico. Dado que todos los que han visto o escrito un currículum tienen una idea bastante clara de lo que contienen, no había ningún misterio sobre lo que hace que una página web sea un currículum. Por lo tanto, el aprendizaje automático no tenía ninguna ventaja sobre el diseño directo de un algoritmo para descubrir currículums.

Otro problema con algunos métodos de aprendizaje automático es que a menudo producen un modelo que, si bien puede ser bastante preciso, no es explicable. En algunos casos, la explicabilidad no es importante. Por ejemplo, si le preguntas a Google por qué ha clasificado un correo de Gmail como spam, suele decir algo como “se parece a otros mensajes que la gente ha identificado como spam”. Es decir, el correo electrónico coincide con cualquier modelo de spam que Google haya desarrollado ese día, sin duda utilizando una técnica del arsenal de algoritmos de aprendizaje automático. Probablemente esa explicación sea satisfactoria. Realmente no nos importa lo que haga Google, siempre y cuando tome la decisión correcta entre spam y no spam.

Por otro lado, considerese una compañía de seguros de automóviles que crea un modelo del riesgo asociado a cada conductor y asigna diferentes primas a cada uno, según el modelo. Si su prima aumenta, es posible que desee una explicación de qué está haciendo el nuevo modelo y por qué cambió la estimación de su riesgo. Desafortunadamente, en muchos métodos de aprendizaje automático, especialmente el "aprendizaje profundo", donde el modelo involucra capa tras capa de pequeños elementos, cada uno de los cuales toma una decisión basada en entradas de la capa anterior, puede que no sea posible dar una explicación coherente de lo que está haciendo el modelo.

## Enfoques computacionales para el modelado
A diferencia del enfoque estadístico, los informáticos tienden a considerar la minería de datos como un problema algorítmico. En este caso, un modelo de datos es simplemente la respuesta a una consulta compleja sobre esos datos. Por ejemplo, dado el conjunto de números del ejemplo anterior, podríamos calcular su promedio y su desviación estándar. Tenga en cuenta que estos valores podrían no ser los parámetros del gaussiano que mejor se ajusten a los datos, aunque es casi seguro que estarán muy cerca si el tamaño de los datos es grande y la fuente de los datos es verdaderamente gaussiana.

Existen muchos enfoques diferentes para modelar datos. Ya hemos mencionado la posibilidad de construir un proceso aleatorio mediante el cual se podrían haber generado los datos. La mayoría de los otros enfoques de modelado pueden describirse como

1. Resumir los datos de forma sucinta y aproximada, o
2. Extraer las características más destacadas de los datos e ignorar el resto.


## Resumen de datos
Una de las formas más interesantes de resumen es la idea del PageRank, que hizo que Google tuviera éxito. En esta forma de minería web, toda la compleja estructura de la Web se resume en un único número para cada página. Este número, el "PageRank" de la página, es (simplificando un poco) la probabilidad de que un caminante aleatorio en el gráfico esté en esa página en un momento dado. Sorprendentemente, esta clasificación refleja muy bien la "importancia" de la página: el grado en que a los buscadores típicos les gustaría que esa página fuera devuelta como respuesta a su consulta de búsqueda.

Otra forma importante de resumen es la agrupación. Aquí, los datos se consideran puntos en un espacio multidimensional. Los puntos que están "cerca" en este espacio se asignan al mismo grupo. Los propios conglomerados se resumen, tal vez dando el centroide del conglomerado y la distancia promedio desde el centroide de los puntos del conglomerado. Estos resúmenes de grupos luego se convierten en el resumen de todo el conjunto de datos.


## Objetivo General de este proyecto
Comparar tres algoritmos de búsqueda de nombres en una base de datos: fuerza bruta, uno basado en estrategia greedy/memoización y otro que utilice funciones de hash.

## Objetivos Específicos
- Crear una base de datos para realizar pruebas de rendimiento de los algoritmos.
- Implementar un algoritmo de "fuerza bruta" para encontrar nombres en la base de datos.
- Crear un mapa de combinaciones de letras que indique el índice donde comenzar y terminar la búsqueda utilizando las letras con las que inicia la palabra.
- Implementar un algoritmo greedy utilizando el mapa de combinaciones de letras.
- Implementar un algoritmo que utilice funciones de hash para identificar si la palabra ya existe en la base de datos.
